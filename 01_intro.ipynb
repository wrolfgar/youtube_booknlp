{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "third-former",
   "metadata": {},
   "source": [
    "# <center>Introduction to BookNLP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-marble",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>March 2022</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-extra",
   "metadata": {},
   "source": [
    "## Key Concepts in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-sperm",
   "metadata": {},
   "source": [
    "1) What is BookNLP?<br>\n",
    "2) How to Install BookNLP<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-cemetery",
   "metadata": {},
   "source": [
    "## About the Author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-cache",
   "metadata": {},
   "source": [
    "I am Dr. William Mattingly. I hold a PhD in Medieval History from the University of Kentucky where I explored early medieval social networks. A lot of my research was aided by my ability to code, specifically in Python, for data cleaning and analysis. I was even able to use Python to plot and visualize social networks and data. During the fourth year of my PhD, I used Python to create an app that I could use to plot and analyze these social networks. Currently, I am a Postdoctoral Fellow for the Analysis of Historical Documents at the Smithsonian Institution with a joint appointment at the United States Holocaust Memorial Museum. In both institutions, I use Python, machine learning, and natural language processing (NLP) to analyze historical texts in large quantities to generate new insights about the documents held in the archives. In all, I have nearly a decade of experience using Python as a historian.\n",
    "\n",
    "When I first started to explore Python, there were not many available tutorials geared towards humanists and, for that reason, four years ago I started PythonHumanities.com and Python Tutorials for Digital Humanities on YouTube. I geared these resources to humanists who had no prior knowledge about computing or coding. This new JupyterBook is the third iteration of this textbook that brings a lot of the material that first appeared on PythonHumanities.com years ago into a new, more accessible JupyterBook. It will forever remain free to all as will the video lectures embedded in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-novel",
   "metadata": {},
   "source": [
    "## About this Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-things",
   "metadata": {},
   "source": [
    "Because this textbook is not peer-reviewed, typos may remain or errors may exist. I openly and freely admit to this. This textbook is community-inspired and I would like it to be community-supported. If you see a mistake, you can click thee GitHub logo inn the top right corner of the screen to submit a pull request or make a note for edits. I highly encourage this and I am open to and welcome any criticism to improve this textbook for all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-marijuana",
   "metadata": {},
   "source": [
    "```{image} ./images/install/github_logo.JPG\n",
    ":alt: jupyter_org\n",
    ":class: bg-primary\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-voice",
   "metadata": {},
   "source": [
    "## What is BookNLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-delicious",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/booknlp/booknlp\">BookNLP</a> is a new Python library created by <a href=\"https://github.com/dbamman\">David Bamman</a>. It was originally created as a Java library in 2014 under the same name, <a href=\"https://github.com/dbamman/book-nlp\">BookNLP</a> by David Bamman, Ted Underwood, and Noah Smith (see, David Bamman, Ted Underwood and Noah Smith, \"A Bayesian Mixed Effects Model of Literary Character,\" ACL 2014). While Java is a powerful coding language, both in speed and ease-of-use, not many digital humanists code in Java primarily. I suspect (I want to emphasize I could be wrong) the reason for the Python library was to address the larger Python-coding community both in general and specifically within the digital humanities. This textbook will deal strictly with the Python library.\n",
    "\n",
    "In the documentation, Bamman states:\n",
    "\n",
    "\"BookNLP is a natural language processing pipeline that scales to books and other long documents (in English), including:\n",
    "\n",
    "- Part-of-speech tagging\n",
    "- Dependency parsing\n",
    "- Entity recognition\n",
    "- Character name clustering (e.g., \"Tom\", \"Tom Sawyer\", \"Mr. Sawyer\", \"Thomas Sawyer\" -> TOM_SAWYER) and coreference resolution\n",
    "- Quotation speaker identification\n",
    "- Supersense tagging (e.g., \"animal\", \"artifact\", \"body\", \"cognition\", etc.)\n",
    "- Event tagging\n",
    "- Referential gender inference (TOM_SAWYER -> he/him/his)\"\n",
    "\n",
    "Unlike its predecessor, the Java library, the Python library leverages the Python NLP library, <a href=\"https://spacy.io/\">spaCy</a>, and the Python Transformer library from <a href=\"https://huggingface.co/\">HuggingFace</a>, rather than Stanford, to perform many of these tasks. In the last few years, spaCy has proven itself as a dominate force within the NLP community, outperforming many of its predecessors in accuracy and in its ability to perform at scale. HuggingFace is a library that allows one to create and leverage large and powerful transformer language models. It also allows users to store these models in the cloud which are too large to store within GitHub or other comparable repositories. I have two textbooks on <a href=\"http://spacy.pythonhumanities.com/intro.html\">spaCy</a>, both for the library generally, and one for <a href=\"http://ner.pythonhumanities.com/intro.html\">named entity recognition</a>, specifically.\n",
    "\n",
    "BookNLP delivers in all the things it sets out to do, though it currently only supports English. Because it leverages transformer models, BookNLP's results can generalize well on non-standard English. I have seen it perform quite well with the South African dialect of English, by correctly identifying out-of-vocabulary (OOV) words, specifically the correct labeling of Afrikaans words for minivans as vehicles.\n",
    "\n",
    "Although only available in English as if March 2022, there are clear plans to expand the library to include Spanish, Japanese, Russian, and Germran, as per their recent <a href=\"https://securegrants.neh.gov/publicquery/main.aspx?f=1&gn=HAA-271654-20\">NEH grant</a>, awarded in September 2020.\n",
    "\n",
    "Throughout this book, we will use BookNLP to do what it was intended to do, analyze large fictional works. We will also, however, push it to analyze larger historical documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-christian",
   "metadata": {},
   "source": [
    "## Why Books and Larger Documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-pricing",
   "metadata": {},
   "source": [
    "Both the documentation and this textbook emphasize the word <i>large</i> here. The reason? Because most language models do not perform well with larger documents. Old RNN-based language models had a hard time remembering earlier words and while newer transformer-based models, such as BERT, have a larger memory and can look forwards and backwards, the size of the input they can take in is only 512 words. For larger documents, therefore, different solutions (and libraries) should be considered. This is where BookNLP comes in. It also addresses several problems associated with books and larger documents, such as:\n",
    "\n",
    "- Characters (and people) are referenced by different names. BookNLP solves this problem with name clustering and coreference resolution. This is a task in NLP where we try and find all uses a name and correctly assign them to the same identifier, such as Harry, Harry Potter, and Mr. Harry Potter all being the same person, Harry Potter.\n",
    "- An adjacent problem is referential gender inferencing. Like coreference resolution, often times in a book or larger document, a person will be referred to as a pronoun. This is where referential gender inferencing comes in. This allows a user to correctly assign the antecedent or postcedent to the correct pronoun. When done successfully, this also allows you to make decisions about the gender of the character or person based on how they are referenced in the text. Because this task is so delicate, given the delicate nature of assigning gender, BookNLP fortunately gives users the data with each pronoun used to reference a character and also includes non-binary pronouns.\n",
    "- Another issue is quotation speaker identification. This is when we need to understand who is speaking, so that we can correctly link characters to their dialogues. It is possible to do this with spaCy, but it is extremely difficult to do well. BookNLP does a remarkable job of handling this problem and it does it with a fair degree of accuracy, from what I have seen.\n",
    "- Event tagging is another key issue with longer documents and books. There are machine learning models that find events and you can easily cultivate a list of domain-specific events to improve a pipeline, but for BookNLP event is defined more broadly. From my experience, it is more based around key actions, rather than named events (as it is in named entity recognition). This has a tangential benefit known as triple extraction. In my opinion, it might be a bit better to view BookNLP events through this lens. Triple extraction is when we try and extract three pieces of information, such as (Actor, Action, Recipient) or (Actor, IS, Something). With these types of tuples, we can construct a knowledge tree about a corpus fairly easily. This a very challenging problem in NLP because triple extraction can be very domain-specific. BookNLP provides a great starting place for triple extraction with its events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-colorado",
   "metadata": {},
   "source": [
    "## How to Install BookNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-match",
   "metadata": {},
   "source": [
    "If you are using Linux (and I believe Mac - for Mac M1, see below), installation will be easy. Use pip install booknlp. You can opt to create a custom environment (recommended but not necessary). If you are using Windows, however, as of March 3 2022, you will need to do a few additional steps which I have documented in this video below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "first-scott",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/3l5ERF3QX0M\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/3l5ERF3QX0M\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7807c-1482-4b7b-9877-90c9d6f7e860",
   "metadata": {},
   "source": [
    "## How to Install BookNLP on Mac M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6808e34-a93a-42f2-834c-1613de939bc1",
   "metadata": {},
   "source": [
    "This section was kindly written by Joel Lee, Fellow at the United States Holocaust Memorial Museum.\n",
    "\n",
    "When running on M1, you may run into a “zsh: illegal hardware instruction” when trying to run spacy or booknlp commands after pip installing booknlp, or you may run into some dependency errors when trying to pip install booknlp. This is my workaround for that, where we download the dependencies (spacy, tensorflow, pytorch, and transformers) stated in the setup.py manually and then running pip install booknlp without its dependencies.\n",
    "\n",
    "- First start with a conda install, we will follow this guide to use miniforge.\n",
    "    - https://www.mrdbourke.com/setup-apple-m1-pro-and-m1-max-for-machine-learning-and-data-science/\n",
    "        - Instant Download Link for Miniforge3 (Conda installer) for macOS arm64 chips (M1, M1 Pro, M1 Max).\n",
    "        - chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh\n",
    "        - sh ~/Downloads/Miniforge3-MacOSX-arm64.sh\n",
    "        - source ~/miniforge3/bin/activate\n",
    "    - Now create a conda environment\n",
    "        - conda create --name booknlp python=3.8\n",
    "        - conda activate booknlp\n",
    "    - Installing tensorflow\n",
    "        - conda install -c apple tensorflow-deps\n",
    "        - python -m pip install tensorflow-macos\n",
    "        - python -m pip install tensorflow-metal\n",
    "- Installing spacy\n",
    "    - conda install -c conda-forge spacy\n",
    "- Installing transformers\n",
    "    - pip install transformers\n",
    "- Installing pytorch\n",
    "    - conda install -c pytorch pytorch\n",
    "- Now if we run pip install booknlp we may get a conflicting dependency error or ResolutionImpossible error between booknlp and tensorflow. Because we have installed the dependencies ourselves, we can run the pip install without the dependencies\n",
    "    - pip install -–no-dependencies booknlp\n",
    "- Now we can download the spacy pipeline and should be able to follow the rest of the booknlp repo.\n",
    "    - python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef1590-6d9c-41c7-9339-3c2ba1cf10fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
